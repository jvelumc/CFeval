---
output: github_document
---
<!-- README.md is generated from README.Rmd. Please edit that file -->
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
set.seed(123)
```
# CFeval <!-- badges: start --> <!-- badges: end -->

Prediction under interventions considers estimating what a subjectâ€™s risk would be if they were to receive a certain treatment. Likewise one may be interested in assessing predictive performance in a setting where all individuals were to receive a certain treatment. This is challenging, as only the outcome of the realized treatment level can be observed in the data, and outcomes under any treatment level are counterfactual.(Keogh, van Geloven, DOI 10.1097/EDE.0000000000001713). This R package facilitates assessing counterfactual predictive performance.

## Installation 
You can install the development version of CFeval from [GitHub](https://github.com/) with:
``` r
# install.packages("devtools")
devtools::install_github("jvelumc/CFeval")
```
## Toy example

Simulate example data for binary outcome Y and (point) treatment A, with the relation between A and Y confounded by a variable L. Variable P is a prognostic variable for only the outcome. The treatment reduces the risk on a bad outcome (Y = 1) in this simulated example. The R package contains a 5000 row df_dev and a 4000 row df_val, both simulated as described.

![Figure 1. DAG for toy example](man/figures/dag.png)

```{r example}
library(CFeval)

simulate_data <- function(n) {
  df <- data.frame(id = 1:n)
  df$L <- rnorm(n)
  df$A <- rbinom(n, 1, plogis(df$L))
  df$P <- rnorm(n)
  df$Y <- rbinom(n, 1, plogis(0.5 + df$L + 1.25 * df$P - 0.6*df$A))
  return(df)
}

set.seed(123)
df_dev <- simulate_data(5000)
df_val <- simulate_data(4000)

head(df_dev)
```

We will use df_dev for model development. The independent df_val will be used to validate the model performance.

Fitting a logistic regression model on this data without accounting for the
confounder L results in a model where treatment apparently increases the risk
on the outcome

```{r}
naive_model <- glm(Y ~ A + P, family = "binomial", data = df_dev)
summary(naive_model)
```

Fitting a model using IP-weighting to account for the confounder results in a
model where treatment decreases the risk on the outcome, which we know to be
true in our simulated data

```{r}

propensity_model <- glm(A ~ L, family = "binomial", df_dev)
prop_score <- predict(propensity_model, type = "response")
prob_trt <- ifelse(df_dev$A == 1, prop_score, 1 - prop_score)
ipw <- 1 / prob_trt

causal_model <- glm(Y ~ A + P, family = "binomial", data = df_dev, 
                    weights = ipw)
summary(causal_model)
```

From now on we assume some model has been developed (be it a good or a bad one), and we want to know if it provides accurate estimates of the counterfactual risk on outcome under both treatment options a = 1 and a = 0.


This package aims to help the user in assessing how well the predictions would match the validation data if all individuals in the validation data had followed a certain treatment option of interest.


The main function CFscore() estimates several counterfactual performance measures in a validation dataset, printing the assumptions required along the way.


```{r}
results_CF0 <- CFscore(
  validation_data = df_val,
  model = list("naive model" = naive_model, "causal model" = causal_model), 
  outcome_column = "Y", 
  propensity_formula = A ~ L, 
  treatment_of_interest = 0
)
results_CF0
```
Weights are exported:
```{r}
summary(results_CF0$ipweights)
```


See also the counterfactual score under treatment option 1

```{r}
results_CF1 <- CFscore(
  validation_data = df_val,
  model = list("naive model" = naive_model, "causal model" = causal_model), 
  outcome_column = "Y", 
  propensity_formula = A ~ L, 
  treatment_of_interest = 0,
  quiet = TRUE
)
results_CF1
```

## Other options

Bootstrapping for 95% confidence intervals (95CI) - in this setting (sample size 4000, binary outcome and treatment, 1 confounder) takes about 30 seconds on my pc with 200 bootstrap iterations. Here we use 50 bootstraps to save time. 
```{r}
CFscore(
  validation_data = df_val,
  model = list("naive model" = naive_model, "causal model" = causal_model), 
  outcome_column = "Y", 
  propensity_formula = A ~ L, 
  treatment_of_interest = 0,
  metrics = c("auc", "brier", "oe"),
  bootstrap = TRUE,
  bootstrap_iterations = 50,
  quiet = TRUE
)
```

We can also give counterfactual predictions to CFscore, instead of models
```{r}

# this is a bit stupid example, defining df_valA0 and then using df_val in CFscore. Makes more sense when you have complicated prediction, such as for time dependent confounding. Don't pay too much attention to this right now.
df_valA0 <- df_val
df_valA0$A <- 0
cf0 <- predict(causal_model, newdata = df_valA0, type = "response")

CFscore(
  validation_data = df_val,
  predictions = cf0,
  outcome_column = "Y", 
  propensity_formula = A ~ L, 
  treatment_of_interest = 0,
  metrics = c("auc", "brier", "oe"),
  quiet = TRUE
)
```

And we can also give it user-specified weights, instead of a propensity formula. 
```{r}
prop_model <- glm(A ~ L, family = "binomial", data = df_val)
prop_score <- predict(prop_model, type = "response")
prob_trt <- ifelse(df_val$A == 1, prop_score, 1 - prop_score)
my_ip_weights <- 1 / prob_trt

CFscore(
  validation_data = df_val,
  predictions = cf0,
  outcome_column = "Y", 
  ipweights = my_ip_weights,
  treatment_column = "A", #need to specify treatment var, which is normally inferred from propensity formula
  treatment_of_interest = 0,
  metrics = c("auc", "brier", "oe"),
  quiet = TRUE
)

```

