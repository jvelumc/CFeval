---
output: github_document
---
<!-- README.md is generated from README.Rmd. Please edit that file -->
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
set.seed(123)
```
# CFeval <!-- badges: start --> <!-- badges: end -->

Predictions under interventions are estimates of what a subject's risk would be
if they were to follow a certain counterfactual treatment. Assessing predictive
performance for these predictions is challenging, as only the outcome of the
realized treatment can be observed.(Keogh, van Geloven, DOI
10.1097/EDE.0000000000001713). This R package facilitates assessing
counterfactual performance of interventional predictions.

## Installation 
You can install the development version of CFeval from [GitHub](https://github.com/) with:
``` r
# install.packages("devtools")
devtools::install_github("jvelumc/CFeval")
```
## Toy example

Simulate some example data for binary outcome Y and (point) treatment A,
confounded by a variable L. Variable P is a prognostic variable for only the
outcome. The treatment reduces the risk on a bad outcome (Y = 1) in this simulated 
example. The R package contains a 5000 row df_dev and a 4000 row df_val, both 
simulated as described. 

![Figure 1. DAG for toy example](man/figures/dag.png)

```{r example}
library(CFeval)
head(df_dev)
```

Fitting a logistic regression model on this data without accounting for the
confounder L results in a model where treatment apparently increases the risk
on the outcome

```{r}
naive_model <- glm(Y ~ A + P, family = "binomial", data = df_dev)
summary(naive_model)
```

Fitting a model using IP-weighting to account for the confounder results in a
model where treatment decreases the risk on the outcome, which we know to be
true in our simulated data

```{r}
causal_model <- glm(Y ~ A + P, family = "binomial", data = df_dev, 
                    weights = ip_weights(df_dev, A ~ L))
summary(causal_model)
```

If either model is to be used to decide on treatment options A, we need accurate
estimates of the counterfactual risk on outcome under both treatment options A =
1 and A = 0. 

Validating a model capable of estimating counterfactual risks is challenging. 
This package aims to guide the user in assessing how well the predictions would 
match the validation data if all individuals had followed the treatment under 
which predictions are made. 

The main function CFscore() estimates these counterfactual performance measures 
in a validation dataset, printing all assumptions required along the way.


```{r}
results_causal <- CFscore(
  data = df_val,
  model = causal_model, 
  Y = "Y", 
  propensity_formula = A ~ L, 
  treatments = list(0, 1)
)
results_causal
```
Weights are exported:
```{r}
summary(results_causal$weights)
```

And calibration plots:
```{r}
plot(results_causal)
```

Compare that to the counterfactual performance of the naive model:
```{r}
results_naive <- CFscore(
  data = df_val,
  model = naive_model,
  Y = "Y",
  propensity_formula = A ~ L,
  treatments = list(0,1),
  quiet = TRUE
)
results_naive
```

```{r}
plot(results_naive)
```

## Other options

Bootstrapping for 95% confidence intervals (95CI) - in this setting (sample size 4000, binary outcome and treatment, 1 confounder) takes about 30 seconds on my pc with 200 bootstrap iterations. Here we use 50 bootstraps to save time. 
```{r}
CFscore(
  data = df_val,
  model = causal_model, 
  Y = "Y", 
  propensity_formula = A ~ L, 
  treatments = list(0, 1),
  bootstrap = 50,
  quiet = TRUE
)
```

Maybe we are only interested in counterfactual performance under level 1
```{r}
CFscore(
  data = df_val,
  model = causal_model, 
  Y = "Y", 
  propensity_formula = A ~ L, 
  treatments = 1,
  quiet = TRUE
)
```

Maybe we have a model for each treatment level
```{r}
df_dev$ip <- ip_weights(df_dev, A ~ L)
model0 <- glm(Y ~ P, family = "binomial", data = df_dev[df_dev$A == 0, ], weights = ip)
model1 <- glm(Y ~ P, family = "binomial", data = df_dev[df_dev$A == 1, ], weights = ip)

CFscore(
  data = df_val,
  model = list(model0, model1),
  Y = "Y",
  propensity_formula = A ~ L,
  treatments = list(0,1),
  quiet = TRUE
)
```
We can also give counterfactual predictions to CFscore, instead of models
```{r}
cf0 <- predict(model0, newdata = df_val, type = "response")
cf1 <- predict(model1, newdata = df_val, type = "response")

CFscore(
  data = df_val,
  predictions = list(cf0, cf1),
  Y = "Y",
  propensity_formula = A ~ L,
  treatments = list(0,1),
  quiet = TRUE
)
```

And we can also give it user-specified weights, instead of a propensity formula. 
```{r}
prop_model <- glm(A ~ L, family = "binomial", data = df_val)
prop_score <- predict(prop_model, type = "response")
prob_trt <- ifelse(df_val$A == 1, prop_score, 1 - prop_score)
my_ip_weights <- 1 / prob_trt
  
CFscore(
  data = df_val,
  predictions = list(cf0, cf1),
  Y = "Y",
  ip = my_ip_weights,
  A = "A", #need to specify treatment var, which is normally inferred from propensity formula
  treatments = list(0,1),
  quiet = TRUE
)
```

We can also assess performance measures on the observed data with the realized treatment values:
```{r}
predictions_naive <- predict(naive_model, type = "response", newdata = df_val)
observed_score(predictions = predictions_naive, Y = df_val$Y)
```
